# Parsing
Parsing is basically making sense of a bunch of tokens. For example, if
we were to design a language where you can assign a variable like this:

```
foo = 1
```

We could say that assignment consist of some word, an equals sign, and a number.
The following are _invalid_ assignments:

```
foo = bar # expects a number on the right hand side of the equation
foo       # no equals, no number
foo =     # nothing here!
= foo     # nothing on the left hand side
```

You can see we only accept a small numbers of tokens sequences. In fact, the
accepted sequences must be carefully ordered in order to be valid. A common
solution to this problem, - matching sequences of characters, are regular
expressions. A not-so-common solution is writing a Parser, implementing a
grammar.

Now, let's introduce a little theory. Don't worry, I promise it won't be that
bad.

A _gramar_ is a set of rules which together define all possible character
sequences accepted as valid. They look like this:

```
RuleName := SomeOtherRule A_TERMINAL
```

Rules can consist only of other rules or a terminal. For example, if we wanted
to match a tiny language `L = { 'a', 'b', 'c' }` we could write:

```
Start := 'a'
       | 'b'
       | 'c'
```

Here `'a'`, `'b'` and `'c'` are all terminals. `|` means _or_. The order in
which the grammar tries the rules is not defined theorically, it just matches
all possible variations. For example:

```
Start := 'ab' A
       | 'aba'
A     := 'a'
```

In that grammar, we have two ways of generating 'aba', one is by using the
first branch of the _or_, and the other is using the second brach.

In our imlementation we'll use a top-down approach and just match the first
branch. The second branch would always be ignored in our implementation, so be
careful with those rules.

Languages generated by a grammar are called _formal languages_, you already
know several formal languages I'm sure, some of them are HTML, XML, CSS,
Javascrit, Ruby, Java, C, etc.

Also, we won't write just any grammar, we'll limit our rules in the grammar a
bit, that way we'll only match Context-Free Languages. Why? Because they
represent the best compromise between power of expression and ease of
implementation. [You can learn more about grammars
here](http://www.cs.nuim.ie/~jpower/Courses/Previous/parsing/node21.html).

Which limits will we define? Not much really, we just need to avoid
left-recursion:

```
Foo = Foo "ab"
    | "ab"
```

A rule which calls itself before calling another rule or a terminal. Why this
limitation? Well, one is because it's harder to implement. Because we'll use
functions to implement rules, the implemenation of a left-recursive rule looks
like this:

```ruby
def my_rule
  if my_rule # infinite loop here
    do something
  else
    do something else
  end
end
```

We've got an infinite loop! The good news is that all grammars with
left-recursion [can be written as a different equivalent grammar
without left-recursion](http://www.csd.uwo.ca/~moreno/CS447/Lectures/Syntax.html/node8.html).
In the next section we'll convert a left-recursive grammar into a
non-left-recursive one.

Just one more thing before we move on, I just want to show you how to to
evaluate a grammar _by hand_. Let's this tiny grammar as an example:

```
Assign     = Identifier EQUALS Number
Identifier = WORD
Number     = NUMBER
```

In the grammar above, I want to match an Identifier rule, a token of type EQUALS
(also known as terminal), and a Number. As you can see, we've defined them using
some building blocks called Terminals or Tokens. In our code, we'll tell the
_WORD_ token to match `[a-z]+` and the _NUMBER_ token will match just [0-9].

To try out this grammar, all we need to know is the substitution model. We just
replace rules with their definition until all we have are terminals. Let's say I
want to match `foo = 1`. We must start from the
initial rule and see if we can get to that:

```
Assign = Identifier EQUALS Number
       = WORD EQUALS NUMBER
       = foo EQUALS NUMBER # foo is a valid workd token, we can replace it
       = foo = NUMBER      # = is a valid equals token
       = foo = 1           # 1 is a valid number token
```

Everything looks good! `foo = 1` was generated, so it belongs to our language.

## On Abstract Syntax Trees
Now, just some more theory before I let you go :) The whole point of the grammar
is to get an Abstract Syntax Tree representation - or AST for short, of our
input. For example, a markdown grammar might parse `hello __world__` as:

```
                       [PARAGRAPH]
                           |
                           v
               +-------[SENTENCES] ---------+
               |             |              |
               v             v              v
          [TEXT="hello "] [BOLD="world"] [TEXT="."]
```

> __NOTE__ If you've never seen a tree data structure before, you might [want to
> check that out](https://en.wikipedia.org/wiki/Tree_(data_structure)).

Our parent node is PARAGRAPH. That node has a single child, SENTENCES, which in
turn has 3 children nodes, TEXT, BOLD and another TEXT. The starting rule in our
parser will be the top-most parent in our tree.

The thing about getting a tree out of a grammar is that we can remove ambiguity.
Consider the following grammar:

```
Start    = Binop
Binop    = Binop Operator Binop
         | Number
Operator = + | - | * | /
Number   = 0 | 1 | 2 | ... | 9
```

If we were to manually build an AST for `2 + 2 - 4`, we get

```
                +------[START]--------+
                |         |           |
                v         v           v
             [BINOP] [OPERATOR=-] [NUMBER=4]
                |
   +------------+----------+
   |            |          |
   v            v          v
[NUMBER=2] [OPERATOR=+] [NUMBER=2]
```

So as you can see, we end up matching `(2 + 2) - 4`. The problem, is that an
equally valid representation could be:

```
   +------[START]----------+
   |          |            |
   v          v            v
[NUMBER=2] [OPERATOR=+] [BINOP]
                           |
              +------------+----------|
              |            |          |
              v            v          v
           [NUMBER=2] [OPERATOR=-] [NUMBER=4]
```

So we end up with `2 + (2 - 4)`. We have two possible ASTs to choose from. We
only need one, but we want our programs to be deterministic, and always return
the same AST for the same input. Looks like we'll have to make some choices.

Luckly for us, because we only use non left-recursive grammars, our grammars
don't have ambiguity! Let's see how we would write this as a non left-recursive
grammar:

```
Start          = Binop
Binop          = Substraction
Substraction   = Adition "-" Binop
Adition        = Division "+" Binop
Division       = Multiplication "/" Binop
Multiplication = Number "*" Binop
Number         = 0 | 1 | 2 | ... | 9
```

As you can see, we explicitly set the order of the operations to be performed.
The order in this case is Multiplication, Division, Adition, Substraction, like
C. The generated AST will now always be the same.

## A simple Markdown grammar
Okay, enough theory! We can now start coding. This is the grammar we'll
implement:

```
Paragraph          := SentenceAndNewline
                    | SentenceAndEOF

SentenceAndNewline := Sentence+ NEWLINE NEWLINE

SentencesAndEOF    := Sentence+ NEWLINE EOF
                    | Sentence+ EOF

Sentence           := EmphasizedText
                    | BoldText
                    | Text

EmphasizedText     := UNDERSCORE BoldText UNDERSCORE

BoldText           := UNDERSCORE UNDERSCORE TEXT UNDERSCORE UNDERSCORE
                        | STAR STAR TEXT STAR STAR
```

As you can see, our tokens are pretty self-explanatory. If you remember from my
previous post, `TEXT` is a token which matches anything but another token,
basically. So it's some kind of _match-all_. 

Our starting rule is the first one, `Paragraph`. A Paragraph is basically a set
of sentences. 

In our markdown language, a `Sentence` is not really a sentence in the english
sense, where it's basically a bunch of words until a full stop. In our language,
`__Hello__.  World` and `Some text. Some more text. Yet more text.` are single,
valid sentences.

## Implementation
Enough theory, let's implement it! The approach we'll take is creating an object
for each rule in the parser. That rule might as well call other objects,
including itself, in order to either accept or reject a sequence of tokens.

For example, the `TEXT` parser matches a single `TEXT` token. 

```
class TextParser < BaseParser
  def match(tokens)
    return Node.null unless tokens.peek('TEXT')
    Node.new(type: 'TEXT', value: tokens.first.value, consumed: 1)
  end
end
```

You can see we return a null node if we could not match something, otherwise,
we return a valid node. We call the result _node_ because we want to build an
abstract syntax tree.

Let's see a parser a bit more complicated:

```
class BoldParser < BaseParser
  def match(tokens)
    return Node.null unless tokens.peek('UNDERSCORE', 'UNDERSCORE', 'TEXT', 'UNDERSCORE', 'UNDERSCORE')
    Node.new(type: 'BOLD', value: tokens.third.value, consumed: 5)
  end
end
```

Once again, we just check the token sequence is valid and return a node. The
emphasis parser is quite similar to this one. What about the sentences? Our
rule was `Sentence := EmphasizedText | BoldText | Text`. Seems simple enough,
we find one of the three possible rules.

```
class SentenceParser < BaseParser
  def match(tokens)
    match_first tokens, emphasis_parser, bold_parser, text_parser
  end
end
```

`match_first` does just that, it will try the given parsers and return the first
valid node it finds. The order of the parsers is very important as they get
tested in the given order.

Now, onto the next rule: `SentenceAndNewline := Sentence+ NEWLINE NEWLINE`.

```ruby
class SentencesAndNewlineParser < BaseParser
  include MatchesStar

  def match(tokens)
    nodes, consumed = match_star tokens, with: sentence_parser
    return Node.null if nodes.empty?
    return Node.null unless tokens.peek_at(consumed, 'NEWLINE', 'NEWLINE')
    consumed += 2 # consume newlines

    SentenceNode.new(sentences: nodes, consumed: consumed)
  end
end
```

As you might see, now instead of trying different parsers, we need to try one
several times. We use a different function, `match_star` which lives on a
concern:

```
module MatchesStar
  # This method tries to match a sentence as many times as possible. It then
  # returns all matched nodes. It's the equivalent of `*`, also known as Kleene
  # star.
  #
  def match_star(tokens, with:)
    matched_nodes = []
    consumed      = 0
    parser        = with

    while true
      node = parser.match(tokens.offset(consumed))
      break if node.null?
      matched_nodes += [node]
      consumed      += node.consumed
    end

    [matched_nodes, consumed]
  end
end
```

The module implements something called _Kleene star_, which is just a fancy way
of saying: "Find this 0 or more times". The equivalent of writing: 
`MyRule = 'a'*`, which would generate `{ '', 'a', 'aa', 'aaa', ... }`.
